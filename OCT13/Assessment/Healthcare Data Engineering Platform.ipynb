{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e18a9d-77c7-4c39-9f20-74decc25213d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.functions import col, year, month, avg, sum, countDistinct, desc\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21db69d-1bfd-4542-bc4a-f59177a973ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1 — Bronze Layer: Raw Ingestion\n",
    "\n",
    "**Tasks:**\n",
    "- Read CSV and JSON data into DataFrames.\n",
    "- Write them as Delta tables ( bronze_patients , bronze_hospitals ,\n",
    "- bronze_appointments )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078f00bf-0d7b-4b8f-a474-328de44992f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV and JSON files from DBFS or mounted storage\n",
    "df_patients = spark.read.option(\"header\", True).csv(\"/FileStore/tables/patients.csv\")\n",
    "df_hospitals = spark.read.option(\"multiline\", True).json(\"/FileStore/tables/hospitals.json\")\n",
    "df_appointments_day1 = spark.read.option(\"header\", True).csv(\"/FileStore/tables/appointments_day1.csv\")\n",
    "\n",
    "# Save to Delta Lake\n",
    "df_patients.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_patients\")\n",
    "df_hospitals.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_hospitals\")\n",
    "df_appointments_day1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_appointments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1819c4b7-364e-4feb-8c8c-ca448c974bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2 — Silver Layer: Data Cleansing & Transformation\n",
    "\n",
    "**Tasks:**\n",
    "- Filter out Pending appointments.\n",
    "- Join patients and hospitals to enrich appointment data.\n",
    "- Add new calculated column: year = year(appointment_date) and month .\n",
    "- Store output as silver_appointments ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4aff0e-5cc9-449b-b5d1-31a99c62f794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load bronze tables\n",
    "bronze_patients = spark.table(\"bronze_patients\")\n",
    "bronze_hospitals = spark.table(\"bronze_hospitals\").withColumnRenamed(\"region\", \"hospital_region\")\n",
    "bronze_appointments = spark.table(\"bronze_appointments\")\n",
    "\n",
    "# Filter completed appointments\n",
    "filtered_appointments = bronze_appointments.filter(col(\"status\") == \"Completed\")\n",
    "\n",
    "# Enrich with patient and hospital data\n",
    "silver_appointments = filtered_appointments \\\n",
    "    .join(bronze_patients, \"patient_id\") \\\n",
    "    .join(bronze_hospitals, \"hospital_id\") \\\n",
    "    .withColumn(\"year\", year(\"appointment_date\")) \\\n",
    "    .withColumn(\"month\", month(\"appointment_date\"))\n",
    "\n",
    "# Write to Silver table\n",
    "silver_appointments.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_appointments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd2dd170-cd6f-4db3-b241-4473306c8b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3 — Gold Layer: Analytical Aggregations\n",
    "### Tasks:\n",
    "- Total revenue per hospital.\n",
    "- Total patients per region.\n",
    "- Top 3 most expensive diagnosis categories.\n",
    "- Store as gold_healthcare_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e22a549-c3f0-428f-b652-ce22652d795a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Revenue Per Hospital\n",
    "gold_revenue = silver_appointments.groupBy(\"hospital_id\", \"hospital_name\") \\\n",
    "    .agg(sum(\"cost\").alias(\"total_revenue\"))\n",
    "\n",
    "gold_revenue.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_revenue_per_hospital\")\n",
    "\n",
    "#2. Patients Per Region\n",
    "gold_patients = silver_appointments.select(\"patient_id\", \"region\").distinct() \\\n",
    "    .groupBy(\"region\").agg(countDistinct(\"patient_id\").alias(\"total_patients\"))\n",
    "\n",
    "gold_patients.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_patients_per_region\")\n",
    "\n",
    "#3. Top 3 Most Expensive Diagnoses\n",
    "gold_diagnosis = silver_appointments.groupBy(\"diagnosis\") \\\n",
    "    .agg(avg(\"cost\").alias(\"avg_cost\")) \\\n",
    "    .orderBy(desc(\"avg_cost\")).limit(3)\n",
    "\n",
    "gold_diagnosis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_top_diagnoses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fe9d2e-0e2d-401e-a182-f3dc9535780e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4 — Incremental Load Simulation\n",
    "**Tasks:**\n",
    "- Create appointments_day2.csv with new data.\n",
    "- Use MERGE or Upsert to update the silver table.\n",
    "- Show how incremental data changes the gold table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f808f773-9ed2-4094-a262-f72db37898ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 279 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\"appointment_id,patient_id,hospital_id,appointment_date,diagnosis,cost,status\n",
    "A1005,P001,H001,2024-02-10,Hypertension,600,Completed\n",
    "A1006,P003,H003,2024-02-11,Heart Disease,1200,Completed\n",
    "A1007,P002,H002,2024-02-15,Flu,200,Pending\n",
    "A1008,P004,H004,2024-02-18,Allergy,350,Completed\n",
    "\"\"\"\n",
    "\n",
    "# Write the data to DBFS file path\n",
    "dbutils.fs.put(\"/FileStore/tables/appointments_day2.csv\", data, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0c05a5-7669-411f-a4ff-e9f996a8f2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# STEP 1: Load new appointments data\n",
    "new_appointments = spark.read.csv(\n",
    "    \"/FileStore/tables/appointments_day2.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# STEP 2: Filter out 'Pending' status\n",
    "new_appointments_filtered = new_appointments.filter(\n",
    "    col(\"status\") != \"Pending\"\n",
    ")\n",
    "\n",
    "# STEP 3: Load bronze tables for enrichment\n",
    "bronze_patients = spark.table(\"bronze_patients\")\n",
    "bronze_hospitals = spark.table(\"bronze_hospitals\").withColumnRenamed(\"region\", \"hospital_region\")\n",
    "\n",
    "# STEP 4: Join & Enrich data with patients and hospitals\n",
    "enriched_new_data = new_appointments_filtered \\\n",
    "    .join(bronze_patients, on=\"patient_id\") \\\n",
    "    .join(bronze_hospitals, on=\"hospital_id\") \\\n",
    "    .withColumn(\"year\", year(\"appointment_date\")) \\\n",
    "    .withColumn(\"month\", month(\"appointment_date\"))\n",
    "\n",
    "# STEP 5: Load the silver Delta table\n",
    "silver_table = DeltaTable.forName(spark, \"silver_appointments\")\n",
    "\n",
    "# STEP 6: Define dictionary for merge column mapping\n",
    "update_dict = {\n",
    "    \"appointment_id\": \"source.appointment_id\",\n",
    "    \"patient_id\": \"source.patient_id\",\n",
    "    \"hospital_id\": \"source.hospital_id\",\n",
    "    \"appointment_date\": \"source.appointment_date\",\n",
    "    \"diagnosis\": \"source.diagnosis\",\n",
    "    \"cost\": \"source.cost\",\n",
    "    \"status\": \"source.status\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"age\": \"source.age\",\n",
    "    \"gender\": \"source.gender\",\n",
    "    \"region\": \"source.region\",\n",
    "    \"hospital_name\": \"source.hospital_name\",\n",
    "    \"hospital_region\": \"source.hospital_region\",\n",
    "    \"year\": \"source.year\",\n",
    "    \"month\": \"source.month\"\n",
    "}\n",
    "\n",
    "# STEP 7: Perform UPSERT (merge)\n",
    "silver_table.alias(\"target\").merge(\n",
    "    enriched_new_data.alias(\"source\"),\n",
    "    \"target.appointment_id = source.appointment_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set=update_dict\n",
    ").whenNotMatchedInsert(\n",
    "    values=update_dict\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7256ec6d-97ee-4566-a406-7882dcd8769f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5 — Delta Lake Features\n",
    "**Tasks:**\n",
    "- Use Time Travel to view the gold table before incremental load.\n",
    "- Use Vacuum to clean up historical versions.\n",
    "- Use Optimize + Z-Ordering on hospital_id ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d46fcb-83b2-492a-b9e9-98941719f2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hospital_id</th><th>hospital_name</th><th>total_revenue</th></tr></thead><tbody><tr><td>H001</td><td>City Care</td><td>400.0</td></tr><tr><td>H004</td><td>CureWell</td><td>300.0</td></tr><tr><td>H002</td><td>LifePlus</td><td>250.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "H001",
         "City Care",
         400.0
        ],
        [
         "H004",
         "CureWell",
         300.0
        ],
        [
         "H002",
         "LifePlus",
         250.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hospital_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hospital_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,recompressionCodec:string,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringQualityPerColumn:array<double>,approxClusteringCoverage:double,compactionType:string,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numAddedFilesWithSameMinMaxOnClusteringColumns:array<bigint>,numClusteringTasksPlanned:int,numClusteringTasksNotPlannedDueToPO:int,numCompactionTasksPlanned:int,numCompactionTasksPlannedUndoneDueToPO:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numLeafNodesCompactedUndoneDueToPO:bigint,numIntermediateNodesCompacted:bigint,numIntermediateNodesCompactedUndoneDueToPO:bigint,totalSizeOfDataToCompactInBytes:bigint,totalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Time Travel\n",
    "display(\n",
    "    spark.sql(\n",
    "        \"SELECT * FROM gold_revenue_per_hospital VERSION AS OF 0\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Vacuum: clean up old versions (recommended minimum is 168 hours)\n",
    "spark.sql(\n",
    "    \"VACUUM gold_revenue_per_hospital RETAIN 168 HOURS\"\n",
    ")\n",
    "\n",
    "# Optimize + Z-ORDER\n",
    "spark.sql(\n",
    "    \"OPTIMIZE gold_revenue_per_hospital ZORDER BY (hospital_name)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c0dd2a-c769-4396-b80f-51902f3e3869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analytical Questions to Solve**\n",
    "1. Total revenue generated by each hospital.\n",
    "2. Average cost per diagnosis category.\n",
    "3. Number of patients served per region.\n",
    "4. Trend of appointments month-over-month.\n",
    "5. Top 5 most expensive treatments in the last 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ae3e76-cb63-4f72-8534-7b43b77f6370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n|hospital_name|total_revenue|\n+-------------+-------------+\n|    City Care|       1000.0|\n|     MediHope|       1200.0|\n|     CureWell|        650.0|\n|     LifePlus|        250.0|\n+-------------+-------------+\n\n+-------------+--------+\n|    diagnosis|avg_cost|\n+-------------+--------+\n| Hypertension|   600.0|\n|Heart Disease|  1200.0|\n|      Allergy|   325.0|\n|     Diabetes|   400.0|\n|          Flu|   250.0|\n+-------------+--------+\n\n+------+--------------+\n|region|total_patients|\n+------+--------------+\n| North|             1|\n|  East|             1|\n|  West|             1|\n| South|             1|\n+------+--------------+\n\n+----+-----+------------------+\n|year|month|total_appointments|\n+----+-----+------------------+\n|2024|    1|                 3|\n|2024|    2|                 3|\n+----+-----+------------------+\n\n+-------------+----------+\n|    diagnosis|total_cost|\n+-------------+----------+\n|Heart Disease|    1200.0|\n|      Allergy|     650.0|\n| Hypertension|     600.0|\n|     Diabetes|     400.0|\n|          Flu|     250.0|\n+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Total revenue generated by each hospital\n",
    "spark.sql(\"SELECT hospital_name, SUM(cost) AS total_revenue FROM silver_appointments GROUP BY hospital_name\").show()\n",
    "\n",
    "#  Average cost per diagnosis\n",
    "spark.sql(\"SELECT diagnosis, AVG(cost) AS avg_cost FROM silver_appointments GROUP BY diagnosis\").show()\n",
    "\n",
    "#  Number of patients served per region\n",
    "spark.sql(\"SELECT region, COUNT(DISTINCT patient_id) AS total_patients FROM silver_appointments GROUP BY region\").show()\n",
    "\n",
    "#  Trend of appointments month-over-month\n",
    "spark.sql(\"SELECT year, month, COUNT(appointment_id) AS total_appointments FROM silver_appointments GROUP BY year, month ORDER BY year, month\").show()\n",
    "\n",
    "#  Top 5 most expensive treatments (last 6 months)\n",
    "spark.sql(\"SELECT diagnosis, SUM(cost) AS total_cost FROM silver_appointments GROUP BY diagnosis ORDER BY total_cost DESC LIMIT 5\").show()\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Healthcare Data Engineering Platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}