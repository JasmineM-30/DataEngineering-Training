{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48fe85c-d7ca-4493-bddc-7ad4891af715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Retail Data Analytics Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7bb6932-1793-4a74-b635-a003308ea06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Bronze Layer â€“ Data Ingestion\n",
    "**Tasks:**\n",
    "- Read CSV, JSON, and other data into DataFrames.\n",
    "- Perform schema inference.\n",
    "- Write raw data into Delta tables ( bronze_customers , bronze_orders ,\n",
    "- bronze_products )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff19d994-3fad-4b3f-995c-a9a1c9257749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- age: string (nullable = true)\n\n+-----------+------------+---------+---+\n|customer_id|        name|     city|age|\n+-----------+------------+---------+---+\n|          1|Rahul Sharma|Bangalore| 28|\n|          2| Priya Singh|    Delhi| 32|\n|          3|  Aman Kumar|Hyderabad| 25|\n|          4| Sneha Reddy|  Chennai| 35|\n|          5| Arjun Mehta|   Mumbai| 30|\n|          6|  Divya Nair|    Delhi| 29|\n+-----------+------------+---------+---+\n\nroot\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- product: string (nullable = true)\n |-- quantity: string (nullable = true)\n |-- price: string (nullable = true)\n |-- status: string (nullable = true)\n |-- order_date: string (nullable = true)\n\n+--------+-----------+----------+--------+-----+---------+----------+\n|order_id|customer_id|   product|quantity|price|   status|order_date|\n+--------+-----------+----------+--------+-----+---------+----------+\n|    1001|          1|    Laptop|       2|55000|Completed|2024-01-15|\n|    1002|          2|    Mobile|       3|25000|Completed|2024-01-16|\n|    1003|          3|      Book|      10|  700|  Pending|2024-01-16|\n|    1004|          1|Headphones|       5| 3000|Completed|2024-01-17|\n+--------+-----------+----------+--------+-----+---------+----------+\n\nroot\n |-- category: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n\n+-----------+----------+------------+\n|   category|product_id|product_name|\n+-----------+----------+------------+\n|Electronics|      P001|      Laptop|\n|Electronics|      P002|      Mobile|\n| Stationery|      P003|        Book|\n|Accessories|      P004|  Headphones|\n+-----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Read raw files with schema inference\n",
    "df_customers = spark.read.option(\"header\", True).csv(\"/FileStore/tables/customers.csv\")\n",
    "df_orders = spark.read.option(\"header\", True).csv(\"/FileStore/tables/orders_day1.csv\")\n",
    "df_products = spark.read.option(\"multiline\", True).json(\"/FileStore/tables/products.json\")\n",
    "\n",
    "#Schema\n",
    "df_customers.printSchema()\n",
    "df_customers.show()\n",
    "\n",
    "df_orders.printSchema()\n",
    "df_orders.show()\n",
    "\n",
    "df_products.printSchema()\n",
    "df_products.show()\n",
    "\n",
    "\n",
    "# Write to Bronze Delta Tables\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_customers\")\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_orders\")\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaef8f38-361f-48fb-b11f-736a40dc1599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Read Bronze tables\n",
    "orders = spark.table(\"bronze_orders\")\n",
    "customers = spark.table(\"bronze_customers\")\n",
    "products = spark.table(\"bronze_products\")\n",
    "\n",
    "# Clean orders: remove pending and nulls\n",
    "clean_orders = orders.filter(\n",
    "    (col(\"status\") == \"Completed\") & (col(\"customer_id\").isNotNull())\n",
    ")\n",
    "\n",
    "# Add total_amount column\n",
    "clean_orders = clean_orders.withColumn(\"total_amount\", col(\"quantity\").cast(\"double\") * col(\"price\").cast(\"double\"))\n",
    "\n",
    "# Join with customer and product using 'product_name' match (adjusting key name if needed)\n",
    "enriched_orders = (\n",
    "    clean_orders\n",
    "    .join(customers, on=\"customer_id\", how=\"inner\")\n",
    "    .join(products, clean_orders[\"product\"] == products[\"product_name\"], \"left\")\n",
    ")\n",
    "\n",
    "# Write Silver Table\n",
    "enriched_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee3ae6d-d379-4681-93b5-5ad5ced6b8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>total_revenue</th></tr></thead><tbody><tr><td>Bangalore</td><td>125000.0</td></tr><tr><td>Delhi</td><td>75000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Bangalore",
         125000.0
        ],
        [
         "Delhi",
         75000.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1117: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product</th><th>total_quantity_sold</th><th>total_revenue</th><th>rank</th></tr></thead><tbody><tr><td>Headphones</td><td>5.0</td><td>15000.0</td><td>1</td></tr><tr><td>Mobile</td><td>3.0</td><td>75000.0</td><td>2</td></tr><tr><td>Laptop</td><td>2.0</td><td>110000.0</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Headphones",
         5.0,
         15000.0,
         1
        ],
        [
         "Mobile",
         3.0,
         75000.0,
         2
        ],
        [
         "Laptop",
         2.0,
         110000.0,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_quantity_sold",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load the Silver table\n",
    "silver_orders = spark.table(\"silver_orders\")\n",
    "\n",
    "#  Calculate total revenue by region\n",
    "revenue_by_region = (\n",
    "    silver_orders\n",
    "    .groupBy(\"city\")\n",
    "    .agg(_sum(\"total_amount\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "display(revenue_by_region)\n",
    "\n",
    "#  Find top-selling products by quantity\n",
    "product_sales = (\n",
    "    silver_orders\n",
    "    .groupBy(\"product\")\n",
    "    .agg(_sum(\"quantity\").alias(\"total_quantity_sold\"))\n",
    ")\n",
    "\n",
    "#  Use window functions to rank products by sales\n",
    "window_spec = Window.orderBy(col(\"total_quantity_sold\").desc())\n",
    "ranked_products = product_sales.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Store final results as gold_sales_summary (including revenue)\n",
    "product_revenue = (\n",
    "    silver_orders\n",
    "    .groupBy(\"product\")\n",
    "    .agg(_sum(\"total_amount\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "# Combine ranking and revenue\n",
    "gold_sales_summary = (\n",
    "    ranked_products\n",
    "    .join(product_revenue, on=\"product\", how=\"left\")\n",
    "    .select(\"product\", \"total_quantity_sold\", \"total_revenue\", \"rank\")\n",
    ")\n",
    "\n",
    "# Save as Delta table in Gold layer\n",
    "gold_sales_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_sales_summary\")\n",
    "\n",
    "# result\n",
    "display(spark.sql(\"SELECT * FROM gold_sales_summary ORDER BY rank LIMIT 10\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66709a43-451b-47ef-b587-797c425273c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "gold_region_revenue = (\n",
    "    spark.table(\"silver_orders\")\n",
    "    .groupBy(\"city\")\n",
    "    .agg(_sum(\"total_amount\").alias(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "gold_region_revenue.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_region_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8943e8-e837-4c34-8a06-7b4fc4a1da5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 227 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\"order_id,customer_id,product,quantity,price,status,order_date\n",
    "1005,4,Mobile,1,25000,Completed,2024-01-18\n",
    "1006,2,Laptop,1,55000,Completed,2024-01-19\n",
    "1007,3,Book,5,700,Completed,2024-01-20\n",
    "1003,3,Book,10,700,Completed,2024-01-16\n",
    "\"\"\"\n",
    "\n",
    "# Write the data to DBFS file path\n",
    "dbutils.fs.put(\"/FileStore/tables/orders_day2.csv\", data, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfceb44-2761-4cc7-9a07-153d98288e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>product</th><th>quantity</th><th>price</th><th>status</th><th>order_date</th><th>total_amount</th></tr></thead><tbody><tr><td>1005</td><td>4</td><td>Mobile</td><td>1</td><td>25000</td><td>Completed</td><td>2024-01-18</td><td>25000</td></tr><tr><td>1006</td><td>2</td><td>Laptop</td><td>1</td><td>55000</td><td>Completed</td><td>2024-01-19</td><td>55000</td></tr><tr><td>1007</td><td>3</td><td>Book</td><td>5</td><td>700</td><td>Completed</td><td>2024-01-20</td><td>3500</td></tr><tr><td>1003</td><td>3</td><td>Book</td><td>10</td><td>700</td><td>Completed</td><td>2024-01-16</td><td>7000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1005,
         4,
         "Mobile",
         1,
         25000,
         "Completed",
         "2024-01-18",
         25000
        ],
        [
         1006,
         2,
         "Laptop",
         1,
         55000,
         "Completed",
         "2024-01-19",
         55000
        ],
        [
         1007,
         3,
         "Book",
         5,
         700,
         "Completed",
         "2024-01-20",
         3500
        ],
        [
         1003,
         3,
         "Book",
         10,
         700,
         "Completed",
         "2024-01-16",
         7000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\": \"yyyy-M-d\"}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>order_id</th><th>product</th><th>quantity</th><th>price</th><th>status</th><th>order_date</th><th>total_amount</th><th>name</th><th>city</th><th>age</th><th>category</th><th>product_id</th><th>product_name</th></tr></thead><tbody><tr><td>1</td><td>1001</td><td>Laptop</td><td>2</td><td>55000</td><td>Completed</td><td>2024-01-15</td><td>110000.0</td><td>Rahul Sharma</td><td>Bangalore</td><td>28</td><td>Electronics</td><td>P001</td><td>Laptop</td></tr><tr><td>2</td><td>1002</td><td>Mobile</td><td>3</td><td>25000</td><td>Completed</td><td>2024-01-16</td><td>75000.0</td><td>Priya Singh</td><td>Delhi</td><td>32</td><td>Electronics</td><td>P002</td><td>Mobile</td></tr><tr><td>3</td><td>1003</td><td>Book</td><td>10</td><td>700</td><td>Completed</td><td>2024-01-16</td><td>7000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1004</td><td>Headphones</td><td>5</td><td>3000</td><td>Completed</td><td>2024-01-17</td><td>15000.0</td><td>Rahul Sharma</td><td>Bangalore</td><td>28</td><td>Accessories</td><td>P004</td><td>Headphones</td></tr><tr><td>4</td><td>1005</td><td>Mobile</td><td>1</td><td>25000</td><td>Completed</td><td>2024-01-18</td><td>25000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>1006</td><td>Laptop</td><td>1</td><td>55000</td><td>Completed</td><td>2024-01-19</td><td>55000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>1007</td><td>Book</td><td>5</td><td>700</td><td>Completed</td><td>2024-01-20</td><td>3500.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "1001",
         "Laptop",
         "2",
         "55000",
         "Completed",
         "2024-01-15",
         110000.0,
         "Rahul Sharma",
         "Bangalore",
         "28",
         "Electronics",
         "P001",
         "Laptop"
        ],
        [
         "2",
         "1002",
         "Mobile",
         "3",
         "25000",
         "Completed",
         "2024-01-16",
         75000.0,
         "Priya Singh",
         "Delhi",
         "32",
         "Electronics",
         "P002",
         "Mobile"
        ],
        [
         "3",
         "1003",
         "Book",
         "10",
         "700",
         "Completed",
         "2024-01-16",
         7000.0,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "1004",
         "Headphones",
         "5",
         "3000",
         "Completed",
         "2024-01-17",
         15000.0,
         "Rahul Sharma",
         "Bangalore",
         "28",
         "Accessories",
         "P004",
         "Headphones"
        ],
        [
         "4",
         "1005",
         "Mobile",
         "1",
         "25000",
         "Completed",
         "2024-01-18",
         25000.0,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "1006",
         "Laptop",
         "1",
         "55000",
         "Completed",
         "2024-01-19",
         55000.0,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "1007",
         "Book",
         "5",
         "700",
         "Completed",
         "2024-01-20",
         3500.0,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read new orders\n",
    "orders_day2 = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/FileStore/tables/orders_day2.csv\")\n",
    ")\n",
    "\n",
    "# Add total_amount column\n",
    "orders_day2 = orders_day2.withColumn(\"total_amount\", expr(\"quantity * price\"))\n",
    "\n",
    "# Filter out Pending orders\n",
    "orders_day2_filtered = orders_day2.filter(\"status != 'Pending'\")\n",
    "\n",
    "display(orders_day2_filtered)\n",
    "\n",
    "# Load silver_orders Delta table\n",
    "silver_orders_delta = DeltaTable.forName(spark, \"silver_orders\")\n",
    "\n",
    "# Define column mapping for update and insert\n",
    "column_mapping = {\n",
    "    \"order_id\": \"updates.order_id\",\n",
    "    \"customer_id\": \"updates.customer_id\",\n",
    "    \"product\": \"updates.product\",\n",
    "    \"quantity\": \"updates.quantity\",\n",
    "    \"price\": \"updates.price\",\n",
    "    \"status\": \"updates.status\",\n",
    "    \"order_date\": \"updates.order_date\",\n",
    "    \"total_amount\": \"updates.total_amount\"\n",
    "}\n",
    "\n",
    "# Perform MERGE with explicit update and insert mappings\n",
    "silver_orders_delta.alias(\"silver\").merge(\n",
    "    orders_day2_filtered.alias(\"updates\"),\n",
    "    \"silver.order_id = updates.order_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set=column_mapping\n",
    ").whenNotMatchedInsert(\n",
    "    values=column_mapping\n",
    ").execute()\n",
    "\n",
    "display(spark.table(\"silver_orders\").orderBy(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777d5858-b14c-493d-828f-c4321abf5c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId         |userName                          |operation                        |operationParameters                                                                                                                                    |job |notebook          |clusterId               |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                          |userMetadata|engineInfo                                |\n+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|4      |2025-10-13 12:11:51|149304925187747|azuser4801_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}|NULL|{1557115414686919}|1013-103756-933iboag-v2n|3          |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 1345, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1345}|NULL        |Databricks-Runtime/17.2.x-photon-scala2.13|\n|3      |2025-10-13 12:11:20|149304925187747|azuser4801_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}|NULL|{1557115414686919}|1013-103756-933iboag-v2n|2          |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 1345, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1345}|NULL        |Databricks-Runtime/17.2.x-photon-scala2.13|\n|2      |2025-10-13 12:09:57|149304925187747|azuser4801_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}|NULL|{1557115414686919}|1013-103756-933iboag-v2n|1          |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 1345, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1345}|NULL        |Databricks-Runtime/17.2.x-photon-scala2.13|\n|1      |2025-10-13 12:09:38|149304925187747|azuser4801_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}|NULL|{1557115414686919}|1013-103756-933iboag-v2n|0          |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 1345, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1345}|NULL        |Databricks-Runtime/17.2.x-photon-scala2.13|\n|0      |2025-10-13 12:08:39|149304925187747|azuser4801_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}|NULL|{1557115414686919}|1013-103756-933iboag-v2n|NULL       |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1345}   |NULL        |Databricks-Runtime/17.2.x-photon-scala2.13|\n+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n+----------+-------------------+-------------+----+\n|   product|total_quantity_sold|total_revenue|rank|\n+----------+-------------------+-------------+----+\n|Headphones|                5.0|      15000.0|   1|\n|    Mobile|                3.0|      75000.0|   2|\n|    Laptop|                2.0|     110000.0|   3|\n+----------+-------------------+-------------+----+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check history to see versions and timestamps\n",
    "spark.sql(\"DESCRIBE HISTORY gold_sales_summary\").show(truncate=False)\n",
    "\n",
    "version_to_query = 0  # change to the desired version number\n",
    "\n",
    "historical_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", version_to_query) \\\n",
    "    .table(\"gold_sales_summary\")\n",
    "\n",
    "historical_df.show()\n",
    "\n",
    "# Overwrite gold_sales_summary with old version to rollback\n",
    "historical_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold_sales_summary\")\n",
    "# Vacuum with default retention (7 days)\n",
    "\n",
    "spark.sql(\"VACUUM gold_sales_summary\")\n",
    "\n",
    "# Or vacuum with custom retention in hours (e.g., 168 hours = 7 days)\n",
    "# spark.sql(\"VACUUM gold_sales_summary RETAIN 168 HOURS\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail Data Analytics Platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}