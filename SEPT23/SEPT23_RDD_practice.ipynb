{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjnXkKlWQgsors0yHvN1zT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4izGMcZ9xz0","executionInfo":{"status":"ok","timestamp":1758620846678,"user_tz":-330,"elapsed":17090,"user":{"displayName":"Jasmine M","userId":"13419308110928796191"}},"outputId":"d6e58615-b2d8-4cdb-f576-7c62681515f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"RDD-Example\").getOrCreate()\n","\n","# Get SparkContext\n","sc = spark.sparkContext"]},{"cell_type":"markdown","source":["---\n","\n","# üìù What is RDD?\n","\n","**RDD** stands for **Resilient Distributed Dataset**.\n","\n","It is the **core data structure of Apache Spark** ‚Äî an **immutable distributed collection of objects** that can be processed in parallel across a cluster.\n","\n","üëâ In simple words:\n","\n","* Think of RDD like a **giant list**, but instead of sitting in one machine, it‚Äôs **spread across multiple machines**.\n","* Spark can apply functions to this distributed data in parallel, making it fast and fault-tolerant.\n","\n","---\n","\n","# üìù Key Properties of RDD\n","\n","* **Resilient** ‚Üí Fault-tolerant (can recover lost data automatically using lineage).\n","* **Distributed** ‚Üí Data is split into partitions across cluster nodes.\n","* **Dataset** ‚Üí Collection of elements (numbers, strings, objects, rows, etc.).\n","* **Immutable** ‚Üí Once created, cannot be changed ‚Äî only transformed into new RDDs.\n","\n","---"],"metadata":{"id":"jb8dEcLN-2sA"}},{"cell_type":"code","source":["# From a Python list\n","data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n","\n","rdd = sc.parallelize(data)\n","\n","print(\"RDD elements:\", rdd.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iG8K-WZO-oW2","executionInfo":{"status":"ok","timestamp":1758621032786,"user_tz":-330,"elapsed":1107,"user":{"displayName":"Jasmine M","userId":"13419308110928796191"}},"outputId":"b0f59fe5-fb73-428c-ebfc-0f1ec9d0da55"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["RDD elements: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"]}]},{"cell_type":"code","source":["# Map: Square each number\n","squared_rad = rdd.map (lambda x: x * x)\n","\n","# Filter: Keep only even numbers\n","even_rdd = rdd.filter (lambda x: x % 2 == 0)"],"metadata":{"id":"iH89vzKhFWJE","executionInfo":{"status":"ok","timestamp":1758623043679,"user_tz":-330,"elapsed":16,"user":{"displayName":"Jasmine M","userId":"13419308110928796191"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print (\"Squared:\", squared_rad.collect())\n","print (\"Even:\", even_rdd.collect())\n","print (\"Count:\", rdd.count())\n","print(\"Sum:\", rdd.sum ())\n","print (\"Max:\", rdd.max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubwNvbYeGIyg","executionInfo":{"status":"ok","timestamp":1758623047128,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Jasmine M","userId":"13419308110928796191"}},"outputId":"2766bb5f-2191-4e40-a3c0-3ba47b3c6d1e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Squared: [1, 4, 9, 16, 25, 36, 49, 64, 81]\n","Even: [2, 4, 6, 8]\n","Count: 9\n","Sum: 45\n","Max: 9\n"]}]},{"cell_type":"code","source":["# Sample text\n","text = [\"hello world\", \"hello spark\", \"big data with spark\"]\n","\n","# Create RDD\n","text_rdd = sc.parallelize(text)\n","\n","# Split words\n","words = text_rdd.flatMap (lambda line: line.split(\" \"))\n","\n","# Map each word to (word, 1)\n","word_pairs = words.map(lambda word: (word, 1))\n","\n","# Reduce by key (sum counts)\n","\n","word_count = word_pairs.reduceByKey(lambda a, b: a + b)\n","\n","# hello,1 --- hello,1\n","print (\"Word Count:\", word_count.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMHt-pq_M-Bl","executionInfo":{"status":"ok","timestamp":1758624788513,"user_tz":-330,"elapsed":1093,"user":{"displayName":"Jasmine M","userId":"13419308110928796191"}},"outputId":"d3cdfc03-0a94-4e38-841d-3f7fbddbcf3e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Count: [('hello', 2), ('world', 1), ('big', 1), ('with', 1), ('spark', 2), ('data', 1)]\n"]}]}]}